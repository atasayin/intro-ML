{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR421\n",
    "## HW 3\n",
    "### ATA SAYIN, 64437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def safelog(x):\n",
    "    return(np.log(x + 1e-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_x=np.genfromtxt(\"hw02_data_set_images.csv\",delimiter=\",\")\n",
    "data_set_y=np.genfromtxt(\"hw02_data_set_labels.csv\",delimiter=\",\",dtype=np.str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/as/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(data_set_y)\n",
    "labelencoder = LabelEncoder()\n",
    "df = labelencoder.fit_transform(df)\n",
    "data_set_y=np.array(df)+1\n",
    "data_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain=int(data_set_x.shape[0]*25/39)\n",
    "ntest=int(data_set_x.shape[0]*14/39)\n",
    "D=data_set_x.shape[1]\n",
    "K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 320)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain=np.vstack([data_set_x[39*i:25+39*i] for i in range(K)])\n",
    "xtest=np.vstack([data_set_x[25+39*i:39+39*i] for i in range(K)])\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain=np.concatenate([data_set_y[39*i:25+39*i] for i in range(K)])\n",
    "ytest=np.concatenate([data_set_y[25+39*i:39+39*i] for i in range(K)])\n",
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 320)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain=np.vstack([data_set_x[39*i:25+39*i] for i in range(K)])\n",
    "xtest=np.vstack([data_set_x[25+39*i:39+39*i] for i in range(K)])\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.zeros((ntrain, K)).astype(int)\n",
    "Y_train[range(ntrain), ytrain - 1] = 1\n",
    "\n",
    "Y_test = np.zeros((ntest, K)).astype(int)\n",
    "Y_test[range(ntest), ytest - 1] = 1\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Finding PCD's and Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.04 0.04 0.04 0.16 0.2  0.16 0.12 0.12 0.24 0.2  0.28\n",
      " 0.36 0.44 0.48 0.56 0.52 0.4  0.   0.04 0.08 0.12 0.16 0.16 0.28 0.28\n",
      " 0.32 0.48 0.56 0.64 0.72 0.76 0.88 0.96 1.   0.92 0.76 0.6  0.   0.04\n",
      " 0.12 0.24 0.28 0.28 0.36 0.44 0.52 0.68 0.8  0.8  0.8  0.88 1.   1.\n",
      " 0.92 0.8  0.76 0.4  0.04 0.16 0.16 0.28 0.44 0.56 0.68 0.76 0.76 0.8\n",
      " 0.76 0.88 0.92 0.92 0.8  0.72 0.68 0.56 0.32 0.2  0.08 0.24 0.36 0.36\n",
      " 0.56 0.64 0.6  0.68 0.68 0.64 0.76 0.8  0.8  0.64 0.48 0.4  0.28 0.12\n",
      " 0.   0.   0.16 0.36 0.44 0.6  0.64 0.72 0.56 0.52 0.48 0.44 0.6  0.68\n",
      " 0.68 0.52 0.4  0.2  0.08 0.   0.   0.   0.32 0.52 0.64 0.64 0.56 0.56\n",
      " 0.4  0.44 0.32 0.44 0.44 0.56 0.64 0.52 0.44 0.12 0.08 0.   0.   0.\n",
      " 0.36 0.64 0.76 0.56 0.56 0.52 0.4  0.4  0.32 0.28 0.36 0.52 0.6  0.44\n",
      " 0.32 0.16 0.16 0.   0.   0.   0.48 0.8  0.76 0.64 0.44 0.36 0.28 0.16\n",
      " 0.2  0.24 0.32 0.48 0.6  0.52 0.4  0.2  0.16 0.   0.   0.   0.6  0.84\n",
      " 0.84 0.72 0.52 0.4  0.32 0.28 0.32 0.28 0.24 0.52 0.56 0.52 0.4  0.2\n",
      " 0.12 0.   0.04 0.   0.56 0.76 0.84 0.76 0.6  0.48 0.4  0.4  0.44 0.36\n",
      " 0.4  0.68 0.6  0.56 0.44 0.32 0.12 0.08 0.08 0.08 0.48 0.68 0.64 0.64\n",
      " 0.56 0.52 0.56 0.52 0.52 0.48 0.64 0.76 0.68 0.52 0.48 0.48 0.4  0.36\n",
      " 0.4  0.36 0.2  0.56 0.52 0.52 0.44 0.56 0.56 0.64 0.6  0.56 0.68 0.8\n",
      " 0.84 0.76 0.68 0.76 0.64 0.64 0.48 0.48 0.2  0.48 0.48 0.48 0.48 0.44\n",
      " 0.48 0.52 0.6  0.68 0.76 0.8  0.8  0.88 0.92 1.   0.88 0.88 0.76 0.72\n",
      " 0.16 0.32 0.36 0.48 0.56 0.6  0.6  0.64 0.64 0.68 0.72 0.8  0.76 0.84\n",
      " 0.88 0.84 0.84 0.76 0.72 0.64 0.08 0.2  0.28 0.36 0.4  0.48 0.48 0.48\n",
      " 0.48 0.52 0.6  0.68 0.72 0.72 0.68 0.56 0.72 0.68 0.68 0.64]\n",
      "-----\n",
      "[0.04 0.24 0.24 0.2  0.12 0.08 0.12 0.16 0.24 0.32 0.28 0.32 0.28 0.28\n",
      " 0.28 0.32 0.4  0.44 0.24 0.2  0.12 0.36 0.48 0.36 0.4  0.52 0.56 0.52\n",
      " 0.6  0.68 0.72 0.68 0.68 0.76 0.76 0.72 0.72 0.68 0.6  0.44 0.28 0.56\n",
      " 0.56 0.52 0.52 0.56 0.56 0.6  0.72 0.76 0.72 0.64 0.72 0.8  0.76 0.76\n",
      " 0.8  0.84 0.68 0.56 0.36 0.64 0.64 0.48 0.44 0.32 0.44 0.52 0.6  0.6\n",
      " 0.64 0.6  0.56 0.52 0.44 0.48 0.64 0.64 0.84 0.76 0.48 0.76 0.56 0.36\n",
      " 0.16 0.2  0.36 0.32 0.56 0.68 0.6  0.44 0.36 0.24 0.16 0.24 0.28 0.4\n",
      " 0.88 0.88 0.48 0.84 0.68 0.28 0.16 0.16 0.36 0.4  0.68 0.76 0.52 0.4\n",
      " 0.28 0.12 0.08 0.08 0.2  0.44 0.88 0.96 0.48 0.84 0.68 0.24 0.08 0.16\n",
      " 0.28 0.4  0.6  0.64 0.44 0.4  0.24 0.08 0.04 0.08 0.12 0.24 0.68 1.\n",
      " 0.52 0.88 0.68 0.24 0.08 0.2  0.36 0.68 0.68 0.64 0.48 0.36 0.16 0.08\n",
      " 0.   0.   0.   0.16 0.56 1.   0.56 0.92 0.72 0.24 0.2  0.28 0.44 0.64\n",
      " 0.72 0.72 0.56 0.36 0.12 0.08 0.   0.   0.04 0.12 0.52 0.96 0.56 0.96\n",
      " 0.8  0.56 0.36 0.4  0.6  0.64 0.72 0.88 0.6  0.36 0.2  0.12 0.08 0.\n",
      " 0.04 0.12 0.56 0.96 0.36 0.8  0.84 0.64 0.6  0.64 0.64 0.56 0.64 0.84\n",
      " 0.64 0.48 0.24 0.12 0.08 0.   0.04 0.24 0.72 0.92 0.28 0.64 0.76 0.64\n",
      " 0.56 0.56 0.6  0.44 0.56 0.8  0.68 0.6  0.32 0.16 0.16 0.08 0.08 0.36\n",
      " 0.64 0.88 0.2  0.44 0.48 0.44 0.48 0.48 0.48 0.36 0.44 0.36 0.68 0.64\n",
      " 0.52 0.4  0.24 0.16 0.2  0.52 0.8  0.76 0.12 0.4  0.44 0.28 0.36 0.44\n",
      " 0.24 0.2  0.2  0.28 0.52 0.76 0.84 0.6  0.36 0.4  0.48 0.84 0.88 0.68\n",
      " 0.12 0.28 0.28 0.28 0.28 0.24 0.16 0.04 0.04 0.12 0.28 0.72 0.88 0.88\n",
      " 0.8  0.84 0.88 0.88 0.68 0.4  0.04 0.16 0.2  0.2  0.16 0.16 0.04 0.\n",
      " 0.   0.04 0.12 0.4  0.72 0.8  0.8  0.88 0.88 0.8  0.56 0.24]\n",
      "-----\n",
      "[0.   0.   0.   0.   0.   0.12 0.2  0.24 0.4  0.56 0.64 0.76 0.72 0.64\n",
      " 0.44 0.36 0.2  0.08 0.   0.   0.   0.   0.12 0.28 0.32 0.56 0.72 0.88\n",
      " 0.92 0.96 0.96 0.96 1.   1.   1.   0.92 0.8  0.48 0.2  0.08 0.   0.2\n",
      " 0.28 0.44 0.6  0.8  0.88 0.92 0.88 0.8  0.76 0.76 0.76 0.88 1.   0.96\n",
      " 0.96 0.84 0.44 0.2  0.04 0.36 0.4  0.64 0.8  0.72 0.68 0.56 0.6  0.44\n",
      " 0.36 0.28 0.36 0.4  0.68 0.8  0.96 0.96 0.8  0.56 0.08 0.4  0.6  0.76\n",
      " 0.76 0.6  0.52 0.36 0.2  0.16 0.08 0.04 0.08 0.12 0.24 0.48 0.88 0.96\n",
      " 0.96 0.68 0.24 0.56 0.8  0.8  0.64 0.44 0.44 0.16 0.12 0.08 0.04 0.\n",
      " 0.   0.   0.08 0.28 0.56 0.96 1.   0.88 0.4  0.72 0.84 0.76 0.56 0.4\n",
      " 0.16 0.12 0.04 0.04 0.   0.   0.   0.   0.   0.04 0.4  0.92 1.   0.96\n",
      " 0.44 0.84 0.92 0.68 0.56 0.28 0.12 0.04 0.04 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.16 0.68 0.96 1.   0.52 0.88 0.88 0.6  0.48 0.16 0.04 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.44 0.92 1.   0.68 0.84\n",
      " 0.84 0.72 0.36 0.2  0.12 0.04 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.04 0.4  0.96 1.   0.64 0.92 0.88 0.68 0.32 0.2  0.16 0.04 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.32 0.88 1.   0.36 0.64 0.8  0.56\n",
      " 0.32 0.24 0.12 0.08 0.   0.   0.   0.   0.   0.   0.   0.   0.16 0.32\n",
      " 0.8  1.   0.36 0.52 0.64 0.48 0.24 0.24 0.16 0.04 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.2  0.4  0.84 1.   0.24 0.48 0.52 0.4  0.2  0.2\n",
      " 0.12 0.04 0.   0.   0.   0.   0.   0.   0.   0.04 0.2  0.48 0.88 0.96\n",
      " 0.16 0.4  0.44 0.36 0.2  0.12 0.08 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.04 0.12 0.24 0.56 0.84 0.84 0.16 0.28 0.32 0.28 0.2  0.16 0.08 0.\n",
      " 0.   0.   0.   0.   0.   0.   0.08 0.16 0.2  0.6  0.88 0.8 ]\n",
      "-----\n",
      "[0.12 0.44 0.4  0.16 0.12 0.08 0.08 0.08 0.12 0.08 0.04 0.04 0.04 0.04\n",
      " 0.04 0.04 0.12 0.32 0.4  0.56 0.44 0.64 0.64 0.44 0.32 0.2  0.24 0.24\n",
      " 0.24 0.28 0.28 0.28 0.32 0.32 0.32 0.44 0.48 0.68 0.68 0.76 0.52 0.76\n",
      " 0.72 0.48 0.4  0.4  0.4  0.44 0.48 0.48 0.48 0.48 0.52 0.48 0.56 0.56\n",
      " 0.6  0.64 0.72 0.8  0.48 0.8  0.76 0.6  0.48 0.32 0.28 0.4  0.4  0.4\n",
      " 0.4  0.48 0.52 0.48 0.4  0.32 0.36 0.52 0.72 0.88 0.44 0.76 0.76 0.68\n",
      " 0.48 0.36 0.28 0.28 0.36 0.4  0.4  0.4  0.4  0.36 0.28 0.32 0.36 0.6\n",
      " 0.76 0.92 0.4  0.8  0.8  0.76 0.44 0.4  0.28 0.24 0.32 0.36 0.28 0.32\n",
      " 0.32 0.32 0.32 0.32 0.32 0.6  0.84 0.96 0.24 0.76 0.76 0.72 0.44 0.32\n",
      " 0.2  0.2  0.24 0.28 0.28 0.4  0.36 0.36 0.32 0.32 0.32 0.48 0.84 0.96\n",
      " 0.16 0.64 0.76 0.72 0.4  0.36 0.36 0.32 0.24 0.24 0.28 0.24 0.16 0.16\n",
      " 0.12 0.16 0.28 0.52 0.84 0.92 0.24 0.6  0.72 0.64 0.52 0.44 0.36 0.28\n",
      " 0.28 0.28 0.24 0.24 0.12 0.12 0.04 0.08 0.2  0.48 0.8  0.92 0.24 0.6\n",
      " 0.72 0.72 0.56 0.36 0.4  0.36 0.28 0.24 0.08 0.08 0.08 0.12 0.16 0.2\n",
      " 0.24 0.64 0.8  0.84 0.16 0.52 0.68 0.72 0.64 0.48 0.4  0.36 0.2  0.08\n",
      " 0.04 0.04 0.12 0.12 0.2  0.2  0.48 0.64 0.84 0.72 0.08 0.32 0.48 0.64\n",
      " 0.56 0.6  0.48 0.24 0.2  0.08 0.08 0.08 0.08 0.08 0.24 0.44 0.6  0.76\n",
      " 0.76 0.64 0.08 0.24 0.44 0.6  0.64 0.6  0.48 0.4  0.24 0.24 0.2  0.16\n",
      " 0.2  0.2  0.36 0.52 0.64 0.76 0.64 0.4  0.08 0.2  0.28 0.36 0.6  0.72\n",
      " 0.68 0.6  0.56 0.48 0.44 0.44 0.44 0.52 0.56 0.64 0.68 0.64 0.48 0.24\n",
      " 0.04 0.16 0.2  0.28 0.48 0.68 0.76 0.8  0.72 0.72 0.68 0.72 0.8  0.8\n",
      " 0.84 0.8  0.72 0.6  0.24 0.16 0.   0.04 0.16 0.24 0.28 0.4  0.56 0.84\n",
      " 0.88 0.92 0.96 0.88 0.8  0.84 0.8  0.64 0.56 0.28 0.16 0.08]\n",
      "-----\n",
      "[0.   0.12 0.12 0.08 0.12 0.16 0.12 0.04 0.12 0.12 0.2  0.2  0.24 0.24\n",
      " 0.32 0.52 0.52 0.56 0.56 0.32 0.   0.16 0.24 0.28 0.32 0.4  0.36 0.44\n",
      " 0.48 0.64 0.6  0.72 0.68 0.8  0.84 0.92 0.92 0.96 0.84 0.68 0.04 0.24\n",
      " 0.36 0.4  0.48 0.48 0.44 0.56 0.56 0.64 0.72 0.8  0.8  0.8  0.76 0.8\n",
      " 0.8  0.96 0.88 0.8  0.04 0.4  0.48 0.56 0.6  0.48 0.6  0.6  0.56 0.6\n",
      " 0.8  0.84 0.88 0.64 0.52 0.44 0.72 0.84 0.96 0.8  0.04 0.48 0.52 0.6\n",
      " 0.56 0.48 0.48 0.52 0.52 0.72 0.76 0.88 0.68 0.4  0.28 0.16 0.4  0.8\n",
      " 0.92 0.88 0.04 0.52 0.68 0.72 0.52 0.36 0.44 0.44 0.4  0.68 0.88 0.84\n",
      " 0.48 0.28 0.04 0.08 0.24 0.8  0.92 0.88 0.16 0.6  0.8  0.84 0.64 0.48\n",
      " 0.28 0.28 0.28 0.68 0.76 0.8  0.4  0.12 0.   0.04 0.32 0.68 0.96 0.88\n",
      " 0.28 0.76 0.88 0.8  0.56 0.32 0.12 0.16 0.28 0.76 0.76 0.84 0.28 0.08\n",
      " 0.04 0.12 0.28 0.72 0.96 0.88 0.28 0.88 0.96 0.64 0.32 0.16 0.04 0.04\n",
      " 0.36 0.76 0.76 0.72 0.32 0.08 0.04 0.12 0.32 0.68 1.   0.88 0.28 1.\n",
      " 1.   0.6  0.28 0.12 0.   0.08 0.36 0.8  0.88 0.72 0.32 0.12 0.04 0.04\n",
      " 0.28 0.64 0.96 0.92 0.4  1.   1.   0.6  0.16 0.12 0.   0.16 0.36 0.8\n",
      " 0.8  0.76 0.28 0.12 0.04 0.04 0.32 0.64 0.92 0.92 0.6  1.   1.   0.48\n",
      " 0.24 0.   0.   0.2  0.4  0.8  0.72 0.6  0.28 0.04 0.   0.04 0.36 0.64\n",
      " 0.92 0.92 0.72 1.   0.96 0.44 0.24 0.   0.   0.16 0.44 0.8  0.76 0.52\n",
      " 0.24 0.04 0.   0.08 0.32 0.56 0.84 0.8  0.68 0.96 0.92 0.48 0.2  0.04\n",
      " 0.04 0.12 0.36 0.68 0.48 0.36 0.2  0.04 0.   0.12 0.32 0.56 0.76 0.64\n",
      " 0.64 0.88 0.76 0.28 0.08 0.   0.04 0.08 0.24 0.4  0.32 0.28 0.12 0.04\n",
      " 0.   0.08 0.32 0.44 0.52 0.52 0.6  0.84 0.64 0.28 0.08 0.   0.04 0.08\n",
      " 0.2  0.24 0.24 0.2  0.08 0.04 0.   0.04 0.28 0.32 0.48 0.44]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "pdc=np.stack([np.mean(xtrain[i*25:25+25*i],axis=0) for i in range(K)])\n",
    "print(pdc[0,])\n",
    "print(\"-----\")\n",
    "print(pdc[1,])\n",
    "print(\"-----\")\n",
    "print(pdc[2,])\n",
    "print(\"-----\")\n",
    "print(pdc[3,])\n",
    "print(\"-----\")\n",
    "print(pdc[4,])\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_priors_train=np.mean(Y_train,axis=0)\n",
    "class_priors_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_priors_test=np.mean(Y_test,axis=0)\n",
    "class_priors_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting PCD's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5)\n",
    "ax1.imshow(pdc[0,].reshape(16,20).T,cmap='binary')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(pdc[1,].reshape(16,20).T,cmap='binary')\n",
    "ax2.axis('off')\n",
    "ax3.imshow(pdc[2,].reshape(16,20).T,cmap='binary')\n",
    "ax3.axis('off')\n",
    "ax4.imshow(pdc[3,].reshape(16,20).T,cmap='binary')\n",
    "ax4.axis('off')\n",
    "ax5.imshow(pdc[4,].reshape(16,20).T,cmap='binary')\n",
    "ax5.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p0=safelog(pdc[0,])\n",
    "p1=safelog(pdc[1,])\n",
    "p2=safelog(pdc[2,])\n",
    "p3=safelog(pdc[3,])\n",
    "p4=safelog(pdc[4,])\n",
    "\n",
    "p=np.array([p0,p1,p2,p3,p4]).T\n",
    "n_p=np.array([safelog(1-pdc[0,]),safelog(1-pdc[1,]),safelog(1-pdc[2,]),safelog(1-pdc[3,]),safelog(1-pdc[4,])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=np.matmul(xtrain,p)+np.matmul(1-xtrain,n_p)+np.array(np.repeat(class_priors_train[None,:],125,axis=0))\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train   1   2   3   4   5\n",
      "y_pred                     \n",
      "1        25   0   0   0   0\n",
      "2         0  24   1   0   1\n",
      "3         0   0  24   0   0\n",
      "4         0   1   0  25   0\n",
      "5         0   0   0   0  24\n"
     ]
    }
   ],
   "source": [
    "y_predicted = np.argmax(y_hat, axis = 1) + 1\n",
    "confusion_matrix = pd.crosstab(y_predicted, ytrain, rownames = ['y_pred'], colnames = ['y_train'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat=np.matmul(xtest,p)+np.matmul(1-xtest,n_p)+np.array(np.repeat(class_priors_test[None,:],70,axis=0))\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train  1   2  3   4   5\n",
      "y_pred                   \n",
      "1        7   0  0   0   0\n",
      "2        0  11  3   2   4\n",
      "3        0   0  7   0   0\n",
      "4        7   3  3  12   0\n",
      "5        0   0  1   0  10\n"
     ]
    }
   ],
   "source": [
    "y_predicted = np.argmax(y_hat, axis = 1) + 1\n",
    "confusion_matrix = pd.crosstab(y_predicted, ytest, rownames = ['y_pred'], colnames = ['y_train'])\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
